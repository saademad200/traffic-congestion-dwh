{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Flow Data Warehouse - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs in-depth exploratory data analysis (EDA) on the traffic flow data to inform data cleaning and transformation decisions for our data warehouse.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#1-setup-and-data-loading)\n",
    "2. [Data Overview and Initial Profiling](#2-data-overview-and-initial-profiling)\n",
    "3. [Traffic Measurements Analysis (Fact Table)](#3-traffic-measurements-analysis-fact-table)\n",
    "4. [Location Dimension Analysis](#4-location-dimension-analysis)\n",
    "5. [Time Dimension Analysis](#5-time-dimension-analysis)\n",
    "6. [Weather Dimension Analysis](#6-weather-dimension-analysis)\n",
    "7. [Event Dimension Analysis](#7-event-dimension-analysis)\n",
    "8. [Vehicle Dimension Analysis](#8-vehicle-dimension-analysis)\n",
    "9. [Infrastructure Dimension Analysis](#9-infrastructure-dimension-analysis)\n",
    "10. [Relationships Between Dimensions](#10-relationships-between-dimensions)\n",
    "11. [Data Quality Assessment](#11-data-quality-assessment)\n",
    "12. [Transformation Recommendations](#12-transformation-recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we'll import the necessary libraries and load our data from the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# To enable interactive plots with Plotly\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# To enable interactive plots with Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to Excel file\n",
    "excel_file_path = 'data/traffic_flow_data.xlsx'\n",
    "\n",
    "# Load data from different sheets\n",
    "traffic_data = pd.read_excel(excel_file_path, sheet_name='Traffic Measurements')\n",
    "location_data = pd.read_excel(excel_file_path, sheet_name='Locations')\n",
    "weather_data = pd.read_excel(excel_file_path, sheet_name='Weather')\n",
    "event_data = pd.read_excel(excel_file_path, sheet_name='Events')\n",
    "vehicle_data = pd.read_excel(excel_file_path, sheet_name='Vehicles')\n",
    "infrastructure_data = pd.read_excel(excel_file_path, sheet_name='Infrastructure')\n",
    "\n",
    "# Standardize column names by converting to lowercase and replacing spaces with underscores\n",
    "for df in [traffic_data, location_data, weather_data, event_data, vehicle_data, infrastructure_data]:\n",
    "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Initial Profiling\n",
    "\n",
    "Let's get a high-level overview of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print dataset information\n",
    "def dataset_overview(df, name):\n",
    "    print(f\"\\n=== {name} Dataset Overview ===\\n\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nFirst 5 rows:\\n{df.head()}\")\n",
    "    print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"\\nBasic Statistics:\\n{df.describe().T}\")\n",
    "    \n",
    "# Call the function for each dataset\n",
    "dataset_overview(traffic_data, \"Traffic Measurements\")\n",
    "dataset_overview(location_data, \"Locations\")\n",
    "dataset_overview(weather_data, \"Weather\")\n",
    "dataset_overview(event_data, \"Events\")\n",
    "dataset_overview(vehicle_data, \"Vehicles\")\n",
    "dataset_overview(infrastructure_data, \"Infrastructure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traffic Measurements Analysis (Fact Table)\n",
    "\n",
    "Let's explore the traffic measurements data in detail. This is our fact table containing the key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime if needed\n",
    "if 'measurement_timestamp' in traffic_data.columns and not pd.api.types.is_datetime64_any_dtype(traffic_data['measurement_timestamp']):\n",
    "    traffic_data['measurement_timestamp'] = pd.to_datetime(traffic_data['measurement_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.histplot(traffic_data['vehicle_count'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Vehicle Count Distribution')\n",
    "\n",
    "sns.histplot(traffic_data['avg_speed'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Average Speed Distribution')\n",
    "\n",
    "sns.histplot(traffic_data['occupancy_rate'], kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Occupancy Rate Distribution')\n",
    "\n",
    "if 'travel_time' in traffic_data.columns:\n",
    "    sns.histplot(traffic_data['travel_time'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Travel Time Distribution')\n",
    "elif 'queue_length' in traffic_data.columns:\n",
    "    sns.histplot(traffic_data['queue_length'], kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Queue Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis of traffic metrics\n",
    "if 'measurement_timestamp' in traffic_data.columns:\n",
    "    # Extract date for grouping\n",
    "    traffic_data['date'] = traffic_data['measurement_timestamp'].dt.date\n",
    "    traffic_data['hour'] = traffic_data['measurement_timestamp'].dt.hour\n",
    "    \n",
    "    # Aggregate by hour for time series\n",
    "    hourly_traffic = traffic_data.groupby('hour').agg({\n",
    "        'vehicle_count': 'mean',\n",
    "        'avg_speed': 'mean',\n",
    "        'occupancy_rate': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Plot time series by hour\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    ax.plot(hourly_traffic['hour'], hourly_traffic['vehicle_count'], label='Vehicle Count')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average Vehicle Count')\n",
    "    ax.set_title('Average Vehicle Count by Hour of Day')\n",
    "    ax.set_xticks(range(0, 24))\n",
    "    plt.show()\n",
    "    \n",
    "    # Multi-metric analysis by hour\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=True)\n",
    "    \n",
    "    ax1.plot(hourly_traffic['hour'], hourly_traffic['avg_speed'], color='green')\n",
    "    ax1.set_ylabel('Average Speed')\n",
    "    ax1.set_title('Average Speed by Hour of Day')\n",
    "    \n",
    "    ax2.plot(hourly_traffic['hour'], hourly_traffic['occupancy_rate'], color='red')\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_ylabel('Occupancy Rate')\n",
    "    ax2.set_title('Average Occupancy Rate by Hour of Day')\n",
    "    ax2.set_xticks(range(0, 24))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for traffic metrics\n",
    "traffic_metrics = ['vehicle_count', 'avg_speed', 'occupancy_rate']\n",
    "\n",
    "# Add other metrics if available\n",
    "for col in ['queue_length', 'travel_time', 'delay_time', 'congestion_index']:\n",
    "    if col in traffic_data.columns:\n",
    "        traffic_metrics.append(col)\n",
    "\n",
    "correlation_matrix = traffic_data[traffic_metrics].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Traffic Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using boxplots\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=traffic_data[traffic_metrics])\n",
    "plt.title('Boxplots for Traffic Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Traffic Data Analysis:\n",
    "\n",
    "1. **Vehicle count distribution** - [Your observations about the distribution]\n",
    "2. **Speed distribution** - [Your observations about the distribution]\n",
    "3. **Occupancy rate patterns** - [Your observations about patterns]\n",
    "4. **Correlations between metrics** - [Your observations about correlations]\n",
    "5. **Temporal patterns** - [Your observations about time patterns]\n",
    "6. **Outliers** - [Your observations about outliers]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]\n",
    "3. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Location Dimension Analysis\n",
    "\n",
    "Let's analyze the location dimension data to understand the geographical distribution of our traffic measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of locations by district\n",
    "if 'district' in location_data.columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    district_counts = location_data['district'].value_counts()\n",
    "    sns.barplot(x=district_counts.index, y=district_counts.values)\n",
    "    plt.title('Number of Locations by District')\n",
    "    plt.xlabel('District')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of road types\n",
    "if 'road_type' in location_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    road_type_counts = location_data['road_type'].value_counts()\n",
    "    sns.barplot(x=road_type_counts.index, y=road_type_counts.values)\n",
    "    plt.title('Distribution of Road Types')\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of lanes and speed limits\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "if 'lanes' in location_data.columns:\n",
    "    sns.countplot(x='lanes', data=location_data, ax=ax1)\n",
    "    ax1.set_title('Distribution of Lane Counts')\n",
    "    ax1.set_xlabel('Number of Lanes')\n",
    "    ax1.set_ylabel('Count')\n",
    "\n",
    "if 'speed_limit' in location_data.columns:\n",
    "    sns.countplot(x='speed_limit', data=location_data, ax=ax2)\n",
    "    ax2.set_title('Distribution of Speed Limits')\n",
    "    ax2.set_xlabel('Speed Limit')\n",
    "    ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize geographical distribution if coordinates are available\n",
    "if 'latitude' in location_data.columns and 'longitude' in location_data.columns:\n",
    "    # Check for valid coordinates\n",
    "    valid_coords = location_data.dropna(subset=['latitude', 'longitude'])\n",
    "    \n",
    "    if not valid_coords.empty:\n",
    "        fig = px.scatter_mapbox(valid_coords, \n",
    "                               lat=\"latitude\", \n",
    "                               lon=\"longitude\", \n",
    "                               hover_name=\"intersection_id\",\n",
    "                               hover_data=[\"street_name\", \"district\", \"road_type\"],\n",
    "                               color=\"road_type\" if \"road_type\" in valid_coords.columns else None,\n",
    "                               size=\"lanes\" if \"lanes\" in valid_coords.columns else None,\n",
    "                               zoom=10,\n",
    "                               height=600)\n",
    "        fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Location Analysis:\n",
    "\n",
    "1. **District distribution** - [Your observations about districts]\n",
    "2. **Road type patterns** - [Your observations about road types]\n",
    "3. **Lane configurations** - [Your observations about lanes]\n",
    "4. **Speed limit distribution** - [Your observations about speed limits]\n",
    "5. **Geographical patterns** - [Your observations about spatial distribution]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Dimension Analysis\n",
    "\n",
    "Let's analyze the temporal patterns in our traffic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time dimension from traffic data timestamps\n",
    "if 'measurement_timestamp' in traffic_data.columns:\n",
    "    time_dim = pd.DataFrame({\n",
    "        'timestamp': pd.Series(traffic_data['measurement_timestamp'].unique()),\n",
    "    })\n",
    "    \n",
    "    time_dim['hour'] = time_dim['timestamp'].dt.hour\n",
    "    time_dim['day'] = time_dim['timestamp'].dt.day\n",
    "    time_dim['day_of_week'] = time_dim['timestamp'].dt.dayofweek\n",
    "    time_dim['day_name'] = time_dim['timestamp'].dt.day_name()\n",
    "    time_dim['month'] = time_dim['timestamp'].dt.month\n",
    "    time_dim['month_name'] = time_dim['timestamp'].dt.month_name()\n",
    "    time_dim['year'] = time_dim['timestamp'].dt.year\n",
    "    \n",
    "    print(f\"Time dimension created with {len(time_dim)} unique timestamps\")\n",
    "    time_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of measurements by day of week\n",
    "plt.figure(figsize=(12, 6))\n",
    "day_counts = time_dim['day_name'].value_counts().reindex([\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "sns.barplot(x=day_counts.index, y=day_counts.values)\n",
    "plt.title('Distribution of Measurements by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of measurements by hour of day\n",
    "plt.figure(figsize=(14, 6))\n",
    "hour_counts = time_dim['hour'].value_counts().sort_index()\n",
    "sns.barplot(x=hour_counts.index, y=hour_counts.values)\n",
    "plt.title('Distribution of Measurements by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly patterns by day of week (heatmap)\n",
    "if 'measurement_timestamp' in traffic_data.columns:\n",
    "    traffic_data['hour'] = traffic_data['measurement_timestamp'].dt.hour\n",
    "    traffic_data['day_of_week'] = traffic_data['measurement_timestamp'].dt.dayofweek\n",
    "    traffic_data['day_name'] = traffic_data['measurement_timestamp'].dt.day_name()\n",
    "    \n",
    "    # Create a pivot table for average vehicle count by day and hour\n",
    "    hourly_by_day = traffic_data.pivot_table(\n",
    "        values='vehicle_count', \n",
    "        index='hour', \n",
    "        columns='day_name',\n",
    "        aggfunc='mean'\n",
    "    ).reindex(columns=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(hourly_by_day, cmap=\"YlOrRd\", annot=True, fmt=\".0f\")\n",
    "    plt.title('Average Vehicle Count by Hour and Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Hour of Day')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Time Analysis:\n",
    "\n",
    "1. **Daily patterns** - [Your observations about daily patterns]\n",
    "2. **Hourly patterns** - [Your observations about hourly patterns]\n",
    "3. **Weekly variations** - [Your observations about weekly variations]\n",
    "4. **Peak times** - [Your observations about peak times]\n",
    "5. **Time gaps** - [Your observations about any gaps in time coverage]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Weather Dimension Analysis\n",
    "\n",
    "Let's analyze the weather data to understand its impact on traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime if needed\n",
    "if 'timestamp' in weather_data.columns and not pd.api.types.is_datetime64_any_dtype(weather_data['timestamp']):\n",
    "    weather_data['timestamp'] = pd.to_datetime(weather_data['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of weather conditions\n",
    "if 'condition' in weather_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    condition_counts = weather_data['condition'].value_counts()\n",
    "    sns.barplot(x=condition_counts.index, y=condition_counts.values)\n",
    "    plt.title('Distribution of Weather Conditions')\n",
    "    plt.xlabel('Weather Condition')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature distribution\n",
    "if 'temperature' in weather_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(weather_data['temperature'], bins=20, kde=True)\n",
    "    plt.title('Temperature Distribution')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather impact on traffic\n",
    "if 'timestamp' in weather_data.columns and 'measurement_timestamp' in traffic_data.columns:\n",
    "    # Merge weather data with traffic data based on nearest timestamp\n",
    "    # This is a simplified approach - in production, you'd use more sophisticated matching\n",
    "    \n",
    "    # Round timestamps to hour to make joining easier\n",
    "    weather_data['hour_timestamp'] = weather_data['timestamp'].dt.floor('H')\n",
    "    traffic_data['hour_timestamp'] = traffic_data['measurement_timestamp'].dt.floor('H')\n",
    "    \n",
    "    # Join datasets\n",
    "    merged_data = pd.merge(traffic_data, weather_data, on='hour_timestamp', how='left')\n",
    "    \n",
    "    # Analyze traffic metrics by weather condition\n",
    "    if 'condition' in merged_data.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='condition', y='avg_speed', data=merged_data)\n",
    "        plt.title('Impact of Weather Conditions on Average Speed')\n",
    "        plt.xlabel('Weather Condition')\n",
    "        plt.ylabel('Average Speed')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='condition', y='vehicle_count', data=merged_data)\n",
    "        plt.title('Impact of Weather Conditions on Vehicle Count')\n",
    "        plt.xlabel('Weather Condition')\n",
    "        plt.ylabel('Vehicle Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between weather metrics and traffic metrics\n",
    "if 'timestamp' in weather_data.columns and 'measurement_timestamp' in traffic_data.columns:\n",
    "    # Use previously merged data\n",
    "    \n",
    "    # Select numeric columns for correlation analysis\n",
    "    numeric_cols = merged_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    \n",
    "    # Filter for relevant columns\n",
    "    relevant_cols = [col for col in numeric_cols if col not in ['hour_timestamp', 'latitude', 'longitude']]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = merged_data[relevant_cols].corr()\n",
    "    \n",
    "    # Focus on correlations between weather and traffic metrics\n",
    "    weather_metrics = ['temperature', 'precipitation', 'visibility', 'wind_speed']\n",
    "    weather_metrics = [col for col in weather_metrics if col in relevant_cols]\n",
    "    \n",
    "    traffic_metrics = ['vehicle_count', 'avg_speed', 'occupancy_rate', 'queue_length', 'travel_time']\n",
    "    traffic_metrics = [col for col in traffic_metrics if col in relevant_cols]\n",
    "    \n",
    "    weather_traffic_corr = correlation_matrix.loc[weather_metrics, traffic_metrics]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(weather_traffic_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation between Weather and Traffic Metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Weather Analysis:\n",
    "\n",
    "1. **Weather condition distribution** - [Your observations about weather conditions]\n",
    "2. **Temperature patterns** - [Your observations about temperature patterns]\n",
    "3. **Weather impact on traffic** - [Your observations about weather impact]\n",
    "4. **Correlations** - [Your observations about correlations]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Event Dimension Analysis\n",
    "\n",
    "Let's analyze event data to understand how events impact traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to datetime if needed\n",
    "for col in ['start_time', 'end_time']:\n",
    "    if col in event_data.columns and not pd.api.types.is_datetime64_any_dtype(event_data[col]):\n",
    "        event_data[col] = pd.to_datetime(event_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of event types\n",
    "if 'event_type' in event_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    event_type_counts = event_data['event_type'].value_counts()\n",
    "    sns.barplot(x=event_type_counts.index, y=event_type_counts.values)\n",
    "    plt.title('Distribution of Event Types')\n",
    "    plt.xlabel('Event Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event duration analysis\n",
    "if 'start_time' in event_data.columns and 'end_time' in event_data.columns:\n",
    "    # Calculate duration\n",
    "    event_data['duration_hours'] = (event_data['end_time'] - event_data['start_time']).dt.total_seconds() / 3600\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(event_data['duration_hours'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Event Duration')\n",
    "    plt.xlabel('Duration (hours)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Event duration by type\n",
    "    if 'event_type' in event_data.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='event_type', y='duration_hours', data=event_data)\n",
    "        plt.title('Event Duration by Type')\n",
    "        plt.xlabel('Event Type')\n",
    "        plt.ylabel('Duration (hours)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event impact on traffic\n",
    "if 'start_time' in event_data.columns and 'measurement_timestamp' in traffic_data.columns:\n",
    "    # Create a time window for each event (simplified approach)\n",
    "    traffic_with_events = traffic_data.copy()\n",
    "    traffic_with_events['has_event'] = False\n",
    "    \n",
    "    # For each event, mark traffic measurements that occurred during the event\n",
    "    for _, event in event_data.iterrows():\n",
    "        if pd.notna(event['start_time']) and pd.notna(event['end_time']):\n",
    "            mask = (traffic_with_events['measurement_timestamp'] >= event['start_time']) & \\\n",
    "                   (traffic_with_events['measurement_timestamp'] <= event['end_time'])\n",
    "            traffic_with_events.loc[mask, 'has_event'] = True\n",
    "    \n",
    "    # Compare traffic metrics with and without events\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='has_event', y='avg_speed', data=traffic_with_events)\n",
    "    plt.title('Impact of Events on Average Speed')\n",
    "    plt.xlabel('Event Occurring')\n",
    "    plt.ylabel('Average Speed')\n",
    "    plt.xticks([0, 1], ['No Event', 'Event Occurring'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='has_event', y='vehicle_count', data=traffic_with_events)\n",
    "    plt.title('Impact of Events on Vehicle Count')\n",
    "    plt.xlabel('Event Occurring')\n",
    "    plt.ylabel('Vehicle Count')\n",
    "    plt.xticks([0, 1], ['No Event', 'Event Occurring'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Event Analysis:\n",
    "\n",
    "### Key Findings from Event Analysis:\n",
    "\n",
    "1. **Event type distribution** - [Your observations about event types]\n",
    "2. **Duration patterns** - [Your observations about event durations]\n",
    "3. **Impact on traffic** - [Your observations about how events affect traffic]\n",
    "4. **Temporal clustering** - [Your observations about when events tend to occur]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vehicle Dimension Analysis\n",
    "\n",
    "Let's analyze the vehicle data to understand the distribution of vehicle types and their impact on traffic metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of vehicle types\n",
    "if 'vehicle_type' in vehicle_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    vehicle_type_counts = vehicle_data['vehicle_type'].value_counts()\n",
    "    sns.barplot(x=vehicle_type_counts.index, y=vehicle_type_counts.values)\n",
    "    plt.title('Distribution of Vehicle Types')\n",
    "    plt.xlabel('Vehicle Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of vehicle classes\n",
    "if 'vehicle_class' in vehicle_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    vehicle_class_counts = vehicle_data['vehicle_class'].value_counts()\n",
    "    sns.barplot(x=vehicle_class_counts.index, y=vehicle_class_counts.values)\n",
    "    plt.title('Distribution of Vehicle Classes')\n",
    "    plt.xlabel('Vehicle Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passenger capacity analysis\n",
    "if 'passenger_capacity' in vehicle_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(vehicle_data['passenger_capacity'], bins=10, kde=True)\n",
    "    plt.title('Distribution of Passenger Capacity')\n",
    "    plt.xlabel('Passenger Capacity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Passenger capacity by vehicle type\n",
    "    if 'vehicle_type' in vehicle_data.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='vehicle_type', y='passenger_capacity', data=vehicle_data)\n",
    "        plt.title('Passenger Capacity by Vehicle Type')\n",
    "        plt.xlabel('Vehicle Type')\n",
    "        plt.ylabel('Passenger Capacity')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Vehicle Analysis:\n",
    "\n",
    "1. **Vehicle type distribution** - [Your observations about vehicle types]\n",
    "2. **Vehicle class patterns** - [Your observations about vehicle classes]\n",
    "3. **Passenger capacity insights** - [Your observations about passenger capacity]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Infrastructure Dimension Analysis\n",
    "\n",
    "Let's analyze the infrastructure data to understand how infrastructure characteristics affect traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date fields to datetime if needed\n",
    "if 'last_maintenance_date' in infrastructure_data.columns and not pd.api.types.is_datetime64_any_dtype(infrastructure_data['last_maintenance_date']):\n",
    "    infrastructure_data['last_maintenance_date'] = pd.to_datetime(infrastructure_data['last_maintenance_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of signal types\n",
    "if 'signal_type' in infrastructure_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    signal_type_counts = infrastructure_data['signal_type'].value_counts()\n",
    "    sns.barplot(x=signal_type_counts.index, y=signal_type_counts.values)\n",
    "    plt.title('Distribution of Signal Types')\n",
    "    plt.xlabel('Signal Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of road conditions\n",
    "if 'road_condition' in infrastructure_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    road_condition_counts = infrastructure_data['road_condition'].value_counts()\n",
    "    sns.barplot(x=road_condition_counts.index, y=road_condition_counts.values)\n",
    "    plt.title('Distribution of Road Conditions')\n",
    "    plt.xlabel('Road Condition')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time since last maintenance\n",
    "if 'last_maintenance_date' in infrastructure_data.columns:\n",
    "    # Calculate days since maintenance\n",
    "    infrastructure_data['days_since_maintenance'] = (pd.Timestamp.now() - infrastructure_data['last_maintenance_date']).dt.days\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(infrastructure_data['days_since_maintenance'].dropna(), bins=20, kde=True)\n",
    "    plt.title('Distribution of Days Since Last Maintenance')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Days since maintenance by road condition\n",
    "    if 'road_condition' in infrastructure_data.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='road_condition', y='days_since_maintenance', data=infrastructure_data)\n",
    "        plt.title('Days Since Maintenance by Road Condition')\n",
    "        plt.xlabel('Road Condition')\n",
    "        plt.ylabel('Days Since Maintenance')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capacity analysis\n",
    "if 'capacity' in infrastructure_data.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(infrastructure_data['capacity'], bins=10, kde=True)\n",
    "    plt.title('Distribution of Infrastructure Capacity')\n",
    "    plt.xlabel('Capacity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Infrastructure Analysis:\n",
    "\n",
    "1. **Signal type distribution** - [Your observations about signal types]\n",
    "2. **Road condition patterns** - [Your observations about road conditions]\n",
    "3. **Maintenance insights** - [Your observations about maintenance patterns]\n",
    "4. **Capacity insights** - [Your observations about capacity distribution]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Relationships Between Dimensions\n",
    "\n",
    "Let's explore how different dimensions interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join relevant dimensions to explore relationships\n",
    "# This is a simplified approach - in production you'd need more sophisticated matching\n",
    "\n",
    "# Join location and infrastructure data\n",
    "if 'intersection_id' in location_data.columns and 'signal_type' in infrastructure_data.columns:\n",
    "    # Assuming there's a common key between them\n",
    "    # For this example, we'll use a random mapping for demonstration\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    location_data['signal_id'] = np.random.choice(infrastructure_data.index, size=len(location_data))\n",
    "    infrastructure_data['infra_id'] = infrastructure_data.index\n",
    "    \n",
    "    location_infra = pd.merge(location_data, infrastructure_data, left_on='signal_id', right_on='infra_id', how='left')\n",
    "    \n",
    "    # Relationship between road type and signal type\n",
    "    if 'road_type' in location_infra.columns and 'signal_type' in location_infra.columns:\n",
    "        cross_tab = pd.crosstab(location_infra['road_type'], location_infra['signal_type'])\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.heatmap(cross_tab, annot=True, cmap='YlGnBu', fmt='d')\n",
    "        plt.title('Road Type vs Signal Type')\n",
    "        plt.ylabel('Road Type')\n",
    "        plt.xlabel('Signal Type')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather impact by location\n",
    "# Assuming we have merged traffic and weather data from earlier analysis\n",
    "\n",
    "try:\n",
    "    # Check if merged_data exists from earlier weather analysis\n",
    "    if 'merged_data' in locals() and 'district' in location_data.columns:\n",
    "        # Add district information to merged data\n",
    "        loc_mapping = {id_val: district for id_val, district in zip(location_data['intersection_id'], location_data['district'])}\n",
    "        if 'location_id' in merged_data.columns:\n",
    "            merged_data['district'] = merged_data['location_id'].map(loc_mapping)\n",
    "            \n",
    "            # Weather impact by district\n",
    "            if 'condition' in merged_data.columns and 'district' in merged_data.columns and 'avg_speed' in merged_data.columns:\n",
    "                plt.figure(figsize=(16, 10))\n",
    "                sns.boxplot(x='district', y='avg_speed', hue='condition', data=merged_data)\n",
    "                plt.title('Impact of Weather Conditions on Speed by District')\n",
    "                plt.xlabel('District')\n",
    "                plt.ylabel('Average Speed')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.legend(title='Weather Condition')\n",
    "                plt.show()\n",
    "except:\n",
    "    print(\"Skipping weather by location analysis due to missing merged data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event impact by road type\n",
    "try:\n",
    "    # Check if traffic_with_events exists from earlier event analysis\n",
    "    if 'traffic_with_events' in locals() and 'road_type' in location_data.columns:\n",
    "        # Add road type information to traffic data\n",
    "        loc_mapping = {id_val: r_type for id_val, r_type in zip(location_data['intersection_id'], location_data['road_type'])}\n",
    "        if 'location_id' in traffic_with_events.columns:\n",
    "            traffic_with_events['road_type'] = traffic_with_events['location_id'].map(loc_mapping)\n",
    "            \n",
    "            # Event impact by road type\n",
    "            if 'has_event' in traffic_with_events.columns and 'road_type' in traffic_with_events.columns and 'avg_speed' in traffic_with_events.columns:\n",
    "                plt.figure(figsize=(16, 10))\n",
    "                sns.boxplot(x='road_type', y='avg_speed', hue='has_event', data=traffic_with_events)\n",
    "                plt.title('Impact of Events on Speed by Road Type')\n",
    "                plt.xlabel('Road Type')\n",
    "                plt.ylabel('Average Speed')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.legend(title='Event Occurring', labels=['No', 'Yes'])\n",
    "                plt.show()\n",
    "except:\n",
    "    print(\"Skipping event by road type analysis due to missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings from Dimension Relationships:\n",
    "\n",
    "1. **Road type and signal type relationship** - [Your observations]\n",
    "2. **Weather impact by location** - [Your observations]\n",
    "3. **Event impact by road type** - [Your observations]\n",
    "\n",
    "### Implications for Data Transformation:\n",
    "\n",
    "1. [Transformation recommendation based on findings]\n",
    "2. [Transformation recommendation based on findings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Quality Assessment\n",
    "\n",
    "Let's assess the quality of our data across all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assess data quality\n",
    "def assess_data_quality(df, name):\n",
    "    print(f\"\\n=== {name} Data Quality Assessment ===\\n\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = 100 * missing / len(df)\n",
    "    missing_df = pd.DataFrame({'Missing Values': missing, 'Percentage': missing_pct})\n",
    "    print(\"Missing Values:\")\n",
    "    print(missing_df[missing_df['Missing Values'] > 0])\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(f\"\\nDuplicate rows: {df.duplicated().sum()} ({100*df.duplicated().sum()/len(df):.2f}%)\")\n",
    "    \n",
    "    # Check for outliers in numeric columns\n",
    "    print(\"\\nPotential Outliers (values > 3 std devs from mean):\")\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = df[(df[col] > mean + 3*std) | (df[col] < mean - 3*std)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"  {col}: {len(outliers)} potential outliers ({100*len(outliers)/len(df):.2f}%)\")\n",
    "    \n",
    "    # Check for consistency issues\n",
    "    print(\"\\nValue Consistency Check:\")\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values < 10:  # Only show for columns with few unique values\n",
    "            print(f\"  {col}: {unique_values} unique values - {df[col].value_counts().head(3).to_dict()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Assess data quality for each dataset\n",
    "assess_data_quality(traffic_data, \"Traffic Measurements\")\n",
    "assess_data_quality(location_data, \"Locations\")\n",
    "assess_data_quality(weather_data, \"Weather\")\n",
    "assess_data_quality(event_data, \"Events\")\n",
    "assess_data_quality(vehicle_data, \"Vehicles\")\n",
    "assess_data_quality(infrastructure_data, \"Infrastructure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Data Quality Issues and Recommendations:\n",
    "\n",
    "1. **Missing Values**\n",
    "   - [List specific columns with significant missing values]\n",
    "   - Recommendation: [Fill with median/mode/interpolated values or drop if necessary]\n",
    "\n",
    "2. **Duplicates**\n",
    "   - [Note any tables with duplicates]\n",
    "   - Recommendation: [Keep first occurrence or aggregate duplicate rows]\n",
    "\n",
    "3. **Outliers**\n",
    "   - [List specific columns with outliers]\n",
    "   - Recommendation: [Capping, transformation, or removal approach]\n",
    "\n",
    "4. **Consistency Issues**\n",
    "   - [Note any inconsistent categorical variables]\n",
    "   - Recommendation: [Standardization approach]\n",
    "\n",
    "5. **Date/Time Issues**\n",
    "   - [Note any issues with timestamps]\n",
    "   - Recommendation: [Standardization approach]\n",
    "\n",
    "6. **Referential Integrity**\n",
    "   - [Note any relationship mismatches between fact and dimension tables]\n",
    "   - Recommendation: [Approach to fix mismatches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Transformation Recommendations\n",
    "\n",
    "Based on our exploratory data analysis, here are the key data transformation recommendations for each dimension and fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Measurements (Fact Table) Transformations\n",
    "\n",
    "1. **Handle Missing Values**\n",
    "   - Fill missing vehicle counts with 0\n",
    "   - Interpolate missing average speeds based on similar time periods\n",
    "   - Set occupancy rate bounds between 0 and 1\n",
    "\n",
    "2. **Handle Outliers**\n",
    "   - Cap vehicle counts at [X value] (3σ from mean)\n",
    "   - Cap average speeds at reasonable maximum (e.g., speed limit + 20%)\n",
    "   - Remove measurements with invalid combinations (e.g., high speed + high occupancy)\n",
    "\n",
    "3. **Derive Additional Metrics**\n",
    "   - Calculate congestion index from speed, occupancy, and vehicle count\n",
    "   - Estimate queue length where missing based on occupancy and speed\n",
    "   - Calculate throughput rate from vehicle count and time interval\n",
    "\n",
    "4. **Temporal Aggregation**\n",
    "   - Create 5-minute, 15-minute, and hourly aggregations for faster analysis\n",
    "   - Include min, max, avg, and std dev in aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Dimension Transformations\n",
    "\n",
    "1. **Complete Time Hierarchy**\n",
    "   - Add season (Spring, Summer, Fall, Winter) based on month\n",
    "   - Add period of day (Morning, Afternoon, Evening, Night) based on hour\n",
    "   - Add fiscal periods if relevant for reporting\n",
    "\n",
    "2. **Special Period Flags**\n",
    "   - Add weekend flag\n",
    "   - Enhance holiday detection with a comprehensive holiday list\n",
    "   - Add peak traffic indicator based on observed patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Dimension Transformations\n",
    "\n",
    "1. **Standardize Geographic Data**\n",
    "   - Validate latitude/longitude values\n",
    "   - Add missing coordinates using geocoding services\n",
    "   - Create standardized district/zone names\n",
    "\n",
    "2. **Enhance with Derived Attributes**\n",
    "   - Add urban/suburban/rural classification\n",
    "   - Create road classification groupings\n",
    "   - Add proximity to key landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Dimension Transformations\n",
    "\n",
    "1. **Standardize Weather Conditions**\n",
    "   - Map similar conditions to standard values (e.g., \"Rain\", \"Drizzle\" → \"Rain\")\n",
    "   - Create weather severity classification\n",
    "\n",
    "2. **Fill Temporal Gaps**\n",
    "   - Interpolate weather for missing time periods\n",
    "   - Create consistent hourly weather records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Dimension Transformations\n",
    "\n",
    "1. **Standardize Event Types**\n",
    "   - Create hierarchical event categories\n",
    "   - Standardize naming conventions\n",
    "\n",
    "2. **Enhance with Derived Attributes**\n",
    "   - Calculate expected impact score based on event size and type\n",
    "   - Add event duration attributes (short, medium, long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Dimension Transformations\n",
    "\n",
    "1. **Standardize Vehicle Classifications**\n",
    "   - Create consistent hierarchy of vehicle types\n",
    "   - Map similar classes to standard values\n",
    "\n",
    "2. **Derive Vehicle Attributes**\n",
    "   - Add vehicle size categories based on passenger capacity\n",
    "   - Add environmentally relevant classifications (electric, hybrid, gas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure Dimension Transformations\n",
    "\n",
    "1. **Standardize Infrastructure Types**\n",
    "   - Create consistent signal type categories\n",
    "   - Standardize road condition descriptions\n",
    "\n",
    "2. **Derive Maintenance Metrics**\n",
    "   - Calculate days since last maintenance\n",
    "   - Create maintenance status category (recent, needed, overdue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exploratory data analysis has provided valuable insights into our traffic flow data warehouse dimensions and facts. The findings will guide our ETL pipeline implementation and help us make informed decisions about data transformations.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Data Quality**: [Summary of overall data quality]\n",
    "2. **Traffic Patterns**: [Summary of important traffic patterns discovered]\n",
    "3. **Dimensional Relationships**: [Summary of important relationships between dimensions]\n",
    "4. **Transformation Priorities**: [List top transformation priorities]\n",
    "\n",
    "Next steps in the ETL implementation:\n",
    "\n",
    "1. Update the transformation logic based on these recommendations\n",
    "2. Implement data quality checks in the pipeline\n",
    "3. Create validation metrics to verify transformation success\n",
    "4. Develop a monitoring framework for ongoing data quality assessment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "couro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
